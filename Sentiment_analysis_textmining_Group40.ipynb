{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27031ebc",
   "metadata": {
    "id": "27031ebc"
   },
   "source": [
    "**Sentiment Analysis Group_40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0d1fa6",
   "metadata": {
    "id": "dc0d1fa6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_data = pd.read_csv('C:/Users/angel/Desktop/Text-Mining-Project-Group-49-main/sentiment-topic-test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf65362",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bf65362",
    "outputId": "6ab3af58-42a3-45ad-db23-c868c61bdf81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence_id', 'sentence', 'sentiment', 'topic']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0610e472-c836-4a94-81d2-b3d4796eade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de09144-4504-412a-8b30-7bd2ebee16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7a8562",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa7a8562",
    "outputId": "792b456a-ea67-4c52-8ae6-d630edafbf62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ff021",
   "metadata": {},
   "source": [
    "**Rule-based sentiment analysis (VADER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e61282",
   "metadata": {
    "id": "72e61282"
   },
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355d8575",
   "metadata": {
    "id": "355d8575"
   },
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5427a1b3",
   "metadata": {
    "id": "5427a1b3"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "all_vader_output = []\n",
    "final = []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    vader_output = run_vader(sentence, lemmatize=True)# run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    all_vader_output.append(vader_label)\n",
    "    final.append(row['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8fdcdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc8fdcdb",
    "outputId": "1574d129-2e2f-430a-8e47-81ba9a32030d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "     neutral       0.20      0.17      0.18         6\n",
      "    positive       0.30      0.50      0.37         6\n",
      "\n",
      "    accuracy                           0.22        18\n",
      "   macro avg       0.17      0.22      0.19        18\n",
      "weighted avg       0.17      0.22      0.19        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use scikit-learn's classification report\n",
    "# Qualitative evaluation\n",
    "print(sklearn.metrics.classification_report(final, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a535f0f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a535f0f3",
    "outputId": "68fec5f5-ac52-4e9b-db70-994e891009e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 should be positive. Misclassified as neutral.\n",
      "Sentence 2 should be positive. Misclassified as neutral.\n",
      "Sentence 11 should be positive. Misclassified as negative.\n",
      "\n",
      "Number of positives misclassified: 3\n",
      "\n",
      "0 The atmosphere at the stadium tonight was electric.\n",
      "2 It had me hooked from the first chapter.\n",
      "11 The author’s writing style is so unique – poetic, but not over the top.\n"
     ]
    }
   ],
   "source": [
    "#Error analysis\n",
    "# Positives misclasified\n",
    "positives_misclasified_indices = []\n",
    "for i in range(len(final)):\n",
    "    if final[i] == \"positive\" and final[i] != all_vader_output[i]:\n",
    "        positives_misclasified_indices.append(i)\n",
    "        print(\"Sentence {} should be {}. Misclassified as {}.\".format(i, final[i], all_vader_output[i]))\n",
    "\n",
    "print(\"\\nNumber of positives misclassified: {}\\n\".format(len(positives_misclasified_indices)))\n",
    "\n",
    "for i in positives_misclasified_indices: print(i, sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ff8066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7ff8066",
    "outputId": "e031b345-0930-4de6-aca7-6b5a53a188ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 7 should be negative. Misclassified as positive.\n",
      "Sentence 10 should be negative. Misclassified as positive.\n",
      "Sentence 12 should be negative. Misclassified as neutral.\n",
      "Sentence 14 should be negative. Misclassified as neutral.\n",
      "Sentence 16 should be negative. Misclassified as positive.\n",
      "Sentence 17 should be negative. Misclassified as positive.\n",
      "\n",
      "Number of negatives misclassified: 6\n",
      "\n",
      "7 How do you concede three goals in ten minutes? The whole defence needs replacing.\n",
      "10 The protagonist was so whiny I wanted to throw the book across the room.\n",
      "12 I don't get how was it supposed to work without any chemistry between the leads.\n",
      "14 I don't get the appeal at all, it's just a couple guys kicking a ball around at the end of the day.\n",
      "16 It's really incredibly impressive to mess up such a tested blockbuster formula.\n",
      "17 The only way it's helped me is by keeping my table from being wobbly.\n"
     ]
    }
   ],
   "source": [
    "# Negatives misclasified\n",
    "positives_misclasified_indices = []\n",
    "for i in range(len(final)):\n",
    "    if final[i] == \"negative\" and final[i] != all_vader_output[i]:\n",
    "        positives_misclasified_indices.append(i)\n",
    "        print(\"Sentence {} should be {}. Misclassified as {}.\".format(i, final[i], all_vader_output[i]))\n",
    "\n",
    "print(\"\\nNumber of negatives misclassified: {}\\n\".format(len(positives_misclasified_indices)))\n",
    "\n",
    "for i in positives_misclasified_indices: print(i, sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58098dbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58098dbb",
    "outputId": "f6bc9e7f-34a1-44fc-fb92-73d61cc9763e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 3 should be neutral. Misclassified as positive.\n",
      "Sentence 4 should be neutral. Misclassified as negative.\n",
      "Sentence 8 should be neutral. Misclassified as positive.\n",
      "Sentence 9 should be neutral. Misclassified as positive.\n",
      "Sentence 13 should be neutral. Misclassified as negative.\n",
      "\n",
      "Number of neutrals misclassified: 5\n",
      "\n",
      "3 It’s more of a slow burn than a page-turner, but it’s well-written, I guess.\n",
      "4 It’s split into two timelines, which keeps it interesting but also a bit confusing at times.\n",
      "8 They rotated their squad for the cup game, which wasn’t surprising given the schedule.\n",
      "9 The trailer gave away most of the plot, but there were still a few surprises.\n",
      "13 It's still 0-0 so far, so way too early to tell - both teams trying their hardest, but maybe it won't be enough?\n"
     ]
    }
   ],
   "source": [
    "# Neutrals misclasified\n",
    "positives_misclasified_indices = []\n",
    "for i in range(len(final)):\n",
    "    if final[i] == \"neutral\" and final[i] != all_vader_output[i]:\n",
    "        positives_misclasified_indices.append(i)\n",
    "        print(\"Sentence {} should be {}. Misclassified as {}.\".format(i, final[i], all_vader_output[i]))\n",
    "\n",
    "print(\"\\nNumber of neutrals misclassified: {}\\n\".format(len(positives_misclasified_indices)))\n",
    "\n",
    "for i in positives_misclasified_indices: print(i, sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615d3eb",
   "metadata": {
    "id": "3615d3eb"
   },
   "source": [
    "**Sentiment analysis with transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ef78d",
   "metadata": {},
   "source": [
    "Link to the pre-trained transformer model: https://huggingface.co/Souvikcmsa/BERT_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a044608-1909-4780-9ede-9f00034d11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd028b30-2eb7-4296-a539-868f16ffd4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.1.0\n",
      "Uninstalling torch-2.1.0:\n",
      "  Successfully uninstalled torch-2.1.0\n",
      "Found existing installation: torchvision 0.16.0\n",
      "Uninstalling torchvision-0.16.0:\n",
      "  Successfully uninstalled torchvision-0.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip uninstall -y torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a665d83-e3cb-4580-afca-ad3b7aa38af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==2.1.0 torchvision==0.16.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dsqoTRcS8PUg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "dsqoTRcS8PUg",
    "outputId": "a96ff3e7-801e-4785-915f-eb938117101b"
   },
   "outputs": [],
   "source": [
    "#!conda install pytorch cpuonly -c pytorch\n",
    "#!pip install transformers\n",
    "#!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f84354d-691d-47a1-8f3f-416acffcabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -q --upgrade accelerate einops xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78e73d5-c5b1-4641-8cad-8bfd7f7494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "PIL.PILLOW_VERSION = PIL.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a915e4b-51a7-4169-a58c-ca7af5e41819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "g37vsxvk_A_0",
   "metadata": {
    "id": "g37vsxvk_A_0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88c578900014bfb93f071977343fdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  77%|#######6  | 336M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\angel\\.cache\\huggingface\\hub\\models--Souvikcmsa--BERT_sentiment_analysis. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\angel\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10591ad1dc2148329a66f05ee8a81011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc5a8034c3e4855b2740fd3795e3d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ace92133f24bd09da8ec6b08015009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741b1e35955f48d6b3bc5b28f5679766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d9646dcaf748b6a24688a4a8ef54f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model = \"Souvikcmsa/BERT_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "HTwiQEFqAjqW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTwiQEFqAjqW",
    "outputId": "722ace67-d05c-4a6b-beb1-81992f33a497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "#TEST: DELETE LATER!!!\n",
    "smth = classifier(test_data['sentence'][0])\n",
    "print(smth[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "TP8eq2mq_Tph",
   "metadata": {
    "id": "TP8eq2mq_Tph"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "all_transformer_output = []\n",
    "final = []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    sentencing = row['sentence']\n",
    "    transformer_output_label = classifier(sentencing)[0]['label']# run transformer\n",
    "    #print(classifier(sentencing))\n",
    "    sentences.append(sentencing)\n",
    "    all_transformer_output.append(transformer_output_label)\n",
    "    final.append(row['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "Byo3UgL0CAM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Byo3UgL0CAM0",
    "outputId": "f1e2dc11-928f-4383-f824-cd96f5528223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.33      0.44         6\n",
      "     neutral       0.40      0.67      0.50         6\n",
      "    positive       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.56      0.50      0.50        18\n",
      "weighted avg       0.56      0.50      0.50        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use scikit-learn's classification report\n",
    "# Qualitative evaluation\n",
    "print(sklearn.metrics.classification_report(final, all_transformer_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4iRYwT8TCUww",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iRYwT8TCUww",
    "outputId": "f654ee5e-7136-49fb-db45-53e058e6231c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 should be positive. Misclassified as neutral.\n",
      "Sentence 1 should be positive. Misclassified as neutral.\n",
      "Sentence 2 should be positive. Misclassified as neutral.\n",
      "\n",
      "Number of positives misclassified: 3\n",
      "\n",
      "0 The atmosphere at the stadium tonight was electric.\n",
      "1 The game was so intense I forgot to breathe at times. What a win!\n",
      "2 It had me hooked from the first chapter.\n"
     ]
    }
   ],
   "source": [
    "#Error analysis\n",
    "# Positives misclasified\n",
    "positives_misclasified_indices = []\n",
    "for i in range(len(final)):\n",
    "    if final[i] == \"positive\" and final[i] != all_transformer_output[i]:\n",
    "        positives_misclasified_indices.append(i)\n",
    "        print(\"Sentence {} should be {}. Misclassified as {}.\".format(i, final[i], all_transformer_output[i]))\n",
    "\n",
    "print(\"\\nNumber of positives misclassified: {}\\n\".format(len(positives_misclasified_indices)))\n",
    "\n",
    "for i in positives_misclasified_indices: print(i, sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "hXcK07XyCepo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXcK07XyCepo",
    "outputId": "91fa5fcc-3b92-482f-d7e3-5fb50dd16a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 7 should be negative. Misclassified as neutral.\n",
      "Sentence 12 should be negative. Misclassified as neutral.\n",
      "Sentence 14 should be negative. Misclassified as neutral.\n",
      "Sentence 16 should be negative. Misclassified as positive.\n",
      "\n",
      "Number of negatives misclassified: 4\n",
      "\n",
      "7 How do you concede three goals in ten minutes? The whole defence needs replacing.\n",
      "12 I don't get how was it supposed to work without any chemistry between the leads.\n",
      "14 I don't get the appeal at all, it's just a couple guys kicking a ball around at the end of the day.\n",
      "16 It's really incredibly impressive to mess up such a tested blockbuster formula.\n"
     ]
    }
   ],
   "source": [
    "# Negatives misclasified\n",
    "positives_misclasified_indices = []\n",
    "for i in range(len(final)):\n",
    "    if final[i] == \"negative\" and final[i] != all_transformer_output[i]:\n",
    "        positives_misclasified_indices.append(i)\n",
    "        print(\"Sentence {} should be {}. Misclassified as {}.\".format(i, final[i], all_transformer_output[i]))\n",
    "\n",
    "print(\"\\nNumber of negatives misclassified: {}\\n\".format(len(positives_misclasified_indices)))\n",
    "\n",
    "for i in positives_misclasified_indices: print(i, sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "EjHaZWk8ChWF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjHaZWk8ChWF",
    "outputId": "0cedfbab-e6b2-44c9-d98a-c18772264c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 3 should be neutral. Misclassified as positive.\n",
      "Sentence 13 should be neutral. Misclassified as negative.\n",
      "\n",
      "Number of neutrals misclassified: 2\n",
      "\n",
      "3 It’s more of a slow burn than a page-turner, but it’s well-written, I guess.\n",
      "13 It's still 0-0 so far, so way too early to tell - both teams trying their hardest, but maybe it won't be enough?\n"
     ]
    }
   ],
   "source": [
    "# Neutrals misclasified\n",
    "positives_misclasified_indices = []\n",
    "for i in range(len(final)):\n",
    "    if final[i] == \"neutral\" and final[i] != all_transformer_output[i]:\n",
    "        positives_misclasified_indices.append(i)\n",
    "        print(\"Sentence {} should be {}. Misclassified as {}.\".format(i, final[i], all_transformer_output[i]))\n",
    "\n",
    "print(\"\\nNumber of neutrals misclassified: {}\\n\".format(len(positives_misclasified_indices)))\n",
    "\n",
    "for i in positives_misclasified_indices: print(i, sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536356b8-122c-44e0-b964-0072de89d51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
